{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The motivation for this Jupyter noteboook is to take snyk output, import it into Elasticsearch, and render some interesting output.  Ordinarily, I would use the snyk REST API, but that is not available on the free tier.  Web scraping is difficult because the site uses OAUTH, and getting Selenium (to render Javascript) working with modified Headers is a tricky exercise.  At least trickier than I had time to solve.\n",
    "\n",
    "In this Jupyter notebook, I take the output from snyk CLI commands and process them with Python.  Part of this processing is to get data into ElasticSearch.  I have a 2-node ElasticSearch cluster on my Ubuntu machine, for some experiments and training.\n",
    "\n",
    "Once the data is in ElasticSearch, I'll do some transformations both with ES and with Python.  For example, on ES I will try some queries and some data representation.  On Python, I'll use the search capabilities of the library to create dataframes and maybe even some plots.\n",
    "\n",
    "I use two types of files.  I'll lead with JSON files from the SnykCLI and use those as inputs into ES.  I'll also generate sarif files.  I expect most work to happen with the JSON files.\n",
    "\n",
    "I tested the following on\n",
    "- Ubuntu 24 LTS \n",
    "- Elasticsearch 8.15\n",
    "- Python 3.12\n",
    "- Snyk CLI 1.1293.1\n",
    "\n",
    "These are the repositories that I used to generate the output files:\n",
    "- git@github.com:marcoman/vulnado.git\n",
    "- git@github.com:marcoman/java-goof.git\n",
    "- git@github.com:marcoman/goof.git\n",
    "\n",
    "These are the containers I used to generate output files:\n",
    "- docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "- A local container built from https://github.com/marcoman/goof/tree/develop/todolist\n",
    "\n",
    "For reference, these are some of the commands I ran to get my files:\n",
    "\n",
    "```bash\n",
    "snyk container test --json --json-file-output=container-elastic.json --app-vulns docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "snyk container test --json --json-file-output=container-todolist-goof.json --app-vulns todolist-goof:latest\n",
    "\n",
    "snyk container test --sarif --sarif-file-output=container-todolist-goof.sarif --app-vulns todolist-goof:latest \n",
    "snyk container test --sarif --sarif-file-output=container-elastic.sarif --app-vulns docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "\n",
    "snyk test --json-file-output=os-goof-todolist.json --json\n",
    "snyk test --json-file-output=os-java-goof.json --json\n",
    "snyk test --json-file-output=os-vulnado.json --json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "import urllib3\n",
    "\n",
    "# This call disables the InsecureRequestWarning for unverified HTTPS requests\n",
    "# This is a common workaround for disabling SSL certificate verification in Python\n",
    "# It should not be used in production environments, as it can lead to security risks\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up our files and start to examine them.  The JSON files come in heavy, and we may have to reduce or only load in a subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to load in files into a variable.  \n",
    "# We *might* use the file contents as-is, but more likely just within a JSON object.\n",
    "json_container_elastic = None\n",
    "json_container_todolist = None\n",
    "json_os_goof_todolist = None\n",
    "json_os_java_goof = None\n",
    "json_os_vulnado = None\n",
    "\n",
    "with open('datafiles/container-elastic.json') as f:\n",
    "    json_container_elastic = json.load(f)\n",
    "\n",
    "with open('datafiles/container-todolist-goof.json') as f:\n",
    "    json_container_todolist = json.load(f)\n",
    "\n",
    "with open('datafiles/os-goof-todolist.json') as f:\n",
    "    json_os_goof_todolist = json.load(f)\n",
    "\n",
    "with open('datafiles/os-java-goof.json') as f:\n",
    "    json_os_java_goof = json.load(f)\n",
    "\n",
    "with open('datafiles/os-vulnado.json') as f:\n",
    "    json_os_vulnado = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part gets our envrionment variables to collect our API credentials.  In my environment, I set these values envvars to help me avoid adding them to the code.  My Elasticsearch server is on my computer, and it is not likely the world will be attacking it.  Still, it is a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ELASTIC_API_URL = os.environ.get('ELASTIC_API_URL')\n",
    "ELASTIC_API_KEY = os.environ.get('ELASTIC_API_KEY')\n",
    "#THe authorization headers are by username + password\n",
    "headers = {\n",
    "    'Authorization': f'ApiKey {ELASTIC_API_KEY}'\n",
    "}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://172.29.213.51:9200/\n"
     ]
    }
   ],
   "source": [
    "print(ELASTIC_API_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Elasticsearch connection\n",
    "\n",
    "As a test, let's see if we access the ES server via a requests call.  This is different from using the ES library, which we'll test later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your products are: \n",
      "{'products': {'aliases': {}, 'mappings': {'properties': {'created': {'type': 'date', 'format': 'yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis'}, 'description': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'id': {'type': 'long'}, 'in_stock': {'type': 'long'}, 'is_active': {'type': 'boolean'}, 'name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'price': {'type': 'long'}, 'sold': {'type': 'long'}, 'tages': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'tags': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '2', 'provided_name': 'products', 'creation_date': '1726425825891', 'number_of_replicas': '2', 'uuid': 'tPcO96JhRLqzI7bhDZSGXQ', 'version': {'created': '8512000'}}}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "## Read the products from the Elastic Server.  This is a GET request to /products\n",
    "def get_products():\n",
    "    url = f\"{ELASTIC_API_URL}/products\"\n",
    "    \n",
    "    # We specify verify=False to match curl's --insecure flag\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "products = get_products()\n",
    "print (f'Your products are: \\n{products}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the snyk cli output files. \n",
    "\n",
    "The general structure of the json file is below.  These are organized with a few differet top-level lists, and we'll spend most of our time focusing on the `vulnerabilities` and `applications` lists.  As I work over the examples, I am expecting to use the `projectName` and `path` as identifiers or query criteria.  This means I'm likely to add all vulnerabilities and application to their respective indicies, and the query will be my filter.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"vulnerabilities\": [],\n",
    "    ...\n",
    "    \"summary\" : \"\",\n",
    "    \"projectName\" : \"\",\n",
    "    \"path\" : \"\",\n",
    "    \"applications\" : [\n",
    "        {\n",
    "            \"projectName\":\"\",\n",
    "            \"dependencyCount\":\"\",\n",
    "            \"displayTargetFile\":\"\",\n",
    "            \"targetFile\":\"\",\n",
    "            \"path\":\"\",\n",
    "            \"packageManager\":\"\",\n",
    "            \"summary\" : \"\",\n",
    "            \"vulnerabilities\":[]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/code/marcoman/marcoman/venv/lib/python3.12/site-packages/elasticsearch/_sync/client/__init__.py:400: SecurityWarning: Connecting to 'https://172.29.213.51:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Elasticsearch client and use the API key to log on.\n",
    "\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(ELASTIC_API_URL, api_key=ELASTIC_API_KEY, verify_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting index applications\n",
      "{'acknowledged': True}\n",
      "Deleting index vulnerabilities\n",
      "{'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "# Clean up indicies to start clean\n",
    "# Delete only if the indicies are present.\n",
    "\n",
    "if es.indices.exists(index='applications'):\n",
    "    print(\"Deleting index applications\")\n",
    "    res = es.indices.delete(index='applications')\n",
    "    print(res)\n",
    "else:\n",
    "    print(\"Index applications does not exist\")\n",
    "\n",
    "if es.indices.exists(index='vulnerabilities'):\n",
    "    print(\"Deleting index vulnerabilities\")\n",
    "    res = es.indices.delete(index='vulnerabilities')\n",
    "    print(res)\n",
    "else:\n",
    "    print(\"Index vulnerabilities does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indicies for our test\n",
    "\n",
    "I create indicies for both `applications` and `vulnerabilities` explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=\"applications\"):\n",
    "    response = es.indices.create(index=\"applications\", body=index_settings)\n",
    "\n",
    "if not es.indices.exists(index=\"vulnerabilities\"):\n",
    "    response = es.indices.create(index=\"vulnerabilities\", body=index_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the indicies\n",
    "\n",
    "The easiest solution is to iterate through the different JSON files and add their data to the indicies.  As I add more, I will automate this even better for the available JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on Elasticsearch container\n",
      "Working on docker-image|docker.elastic.co/elasticsearch/elasticsearch with docker.elastic.co/elasticsearch/elasticsearch:8.15.1/elasticsearch/elasticsearch\n",
      "There are 94 applications\n",
      "There are 104 vulnerabilities\n",
      "Operating on Java TODO List container\n",
      "Working on docker-image|todolist-goof with todolist-goof:latest\n",
      "There are 7 applications\n",
      "There are 2293 vulnerabilities\n",
      "Operating on Goof Open Source\n",
      "Working on io.github.snyk:todolist-mvc with /home/marco/code/marcoman/goof/todolist and maven\n",
      "There are 0 vulnerabilities\n",
      "Operating on Java Goof Open Source\n",
      "Working on io.github.snyk:java-goof with /home/marco/code/marcoman/java-goof and maven\n",
      "There are 0 vulnerabilities\n",
      "Operating on Vulnado Open Source\n",
      "Working on com.scalesec:vulnado with /home/marco/code/marcoman/vulnado and maven\n",
      "There are 126 vulnerabilities\n"
     ]
    }
   ],
   "source": [
    "json_files = [[\"Elasticsearch container\", json_container_elastic, \"container\"],\n",
    "              [\"Java TODO List container\", json_container_todolist, \"container\"],\n",
    "              [\"Goof Open Source\", json_os_goof_todolist, \"opensource\"],\n",
    "              [\"Java Goof Open Source\", json_os_java_goof, \"opensource\"],\n",
    "              [\"Vulnado Open Source\", json_os_vulnado, \"opensource\"],\n",
    "              ]\n",
    "\n",
    "def iterate_through_containers(jsonfile):\n",
    "    # we want to iterate and report on two lists inside of the Json body named jsonfile.\n",
    "    # The first is named applications, and the second is named vulnerabilities.  \n",
    "    # These two lists are independent and at the same level\n",
    "    # Read through each and print out their contents\n",
    "    print(f'Operating on {jsonfile[0]}')\n",
    "    projectName = jsonfile[1]['projectName']\n",
    "    path = jsonfile[1]['path']\n",
    "    \n",
    "    print(f'Working on {projectName} with {path}')\n",
    "\n",
    "    i = 0\n",
    "    for app in jsonfile[1]['applications']:\n",
    "        # print(app)\n",
    "        # Now load each app named \"app\" as a new document in ElasticSearch into the index named \"applications\"\n",
    "        # There is variation in the records and I need to adjust how they are stored.  For example, the upgradePath is empty or contains values.async_search\n",
    "        # For this part, I'll create a new record that is just a subset of the original.\n",
    "        if \"vulnerable dependency path\" in app['summary']:\n",
    "            # We know the first token is a number, so let's get it and render it as an integer\n",
    "            vulns = int(app['summary'].split(' ')[0])\n",
    "        else:\n",
    "            vulns = 0\n",
    "            \n",
    "        newapp = {\n",
    "            \"projectName\" : projectName,\n",
    "            \"path\" : path,\n",
    "            \"appProjectName\": app['projectName'],\n",
    "            \"targetFile\": app['targetFile'],\n",
    "            \"summary\": app['summary'],\n",
    "            \"displayTargetFile\": app['displayTargetFile'],\n",
    "            \"id\" : i,\n",
    "            \"vulns\" : vulns\n",
    "        }\n",
    "        es.index(index=\"applications\", document=newapp)\n",
    "        i += 1\n",
    "    print(f'There are {i} applications')\n",
    "\n",
    "    i = 0        \n",
    "    for vuln in jsonfile[1]['vulnerabilities']:\n",
    "        # print(vuln)\n",
    "        newvuln = {\n",
    "            \"projectName\" : projectName,\n",
    "            \"path\" : path,\n",
    "            \"id\": vuln['id'],\n",
    "            \"CVSSv3\": vuln['CVSSv3'],\n",
    "            \"cvssScore\": vuln['cvssScore'],\n",
    "            \"description\": vuln['description'],\n",
    "            \"packageName\": vuln['packageName'],\n",
    "        }\n",
    "        es.index(index=\"vulnerabilities\", document=newvuln)\n",
    "        i += 1\n",
    "    print(f'There are {i} vulnerabilities')\n",
    "    \n",
    "def iterate_through_opensource(jsonfile):\n",
    "    print(f'Operating on {jsonfile[0]}')\n",
    "    projectName = jsonfile[1]['projectName']\n",
    "    path = jsonfile[1]['path']\n",
    "    packageManager = jsonfile[1]['packageManager']\n",
    "    print(f'Working on {projectName} with {path} and {packageManager}')\n",
    "    i = 0\n",
    "    for vuln in jsonfile[1]['vulnerabilities']:\n",
    "        newvuln = {\n",
    "            \"projectName\" : projectName,\n",
    "            \"path\" : path,\n",
    "            \"packageManager\" : packageManager,\n",
    "            \"CVSSv3\": vuln['CVSSv3'],\n",
    "            \"cvssScore\": vuln['cvssScore'],\n",
    "            \"id\": vuln['id'],\n",
    "            \"language\" : vuln['language'],\n",
    "            \"packageName\" : vuln['packageName'],\n",
    "            \"severity\": vuln['severity'],\n",
    "            \"description\": vuln['description'],\n",
    "            \"packageName\": vuln['packageName'],\n",
    "        }\n",
    "        es.index(index=\"vulnerabilities\", document=newvuln)\n",
    "        i += 1\n",
    "    print(f'There are {i} vulnerabilities')\n",
    "\n",
    "for jsonfile in json_files:\n",
    "    if jsonfile[2] == \"container\":\n",
    "        iterate_through_containers(jsonfile=jsonfile)\n",
    "    elif jsonfile[2] == \"opensource\":\n",
    "        iterate_through_opensource(jsonfile=jsonfile)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported vulnerabilities from a few container and a few open-source projects, let's see what extra details or insights we can get.\n",
    "\n",
    "NOTE: Some projects did not have vulnerabilities.  Also, with the free tier, we may be limited in the total number of scans we can run.\n",
    "\n",
    "Let's start by getting a count of records that match our search criteria.  Since all of the data is in two different indicies, I will be looking for unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique project names in applications index:\n",
      "docker-image|docker.elastic.co/elasticsearch/elasticsearch\n",
      "docker-image|todolist-goof\n",
      "\n",
      "Unique project names in vulnerabilities index:\n",
      "docker-image|todolist-goof\n",
      "com.scalesec:vulnado\n",
      "docker-image|docker.elastic.co/elasticsearch/elasticsearch\n"
     ]
    }
   ],
   "source": [
    "# Let's get the number of unique projectName values in the applications and vulnerabiliities indicies\n",
    "# We will use query results from elasticsearch.\n",
    "\n",
    "# Define the aggregation query\n",
    "query = {\n",
    "    \"size\": 0,\n",
    "    \"aggs\": {\n",
    "        \"unique_project_names\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"projectName.keyword\",\n",
    "                \"size\": 10000  # Adjust the size as needed\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the search query\n",
    "response = es.search(index=\"applications\", body=query)\n",
    "\n",
    "# Extract the unique project names\n",
    "app_project_names = [bucket['key'] for bucket in response['aggregations']['unique_project_names']['buckets']]\n",
    "\n",
    "# Print the unique project names\n",
    "print(\"\\nUnique project names in applications index:\")\n",
    "for project_name in app_project_names:\n",
    "    print(project_name)\n",
    "\n",
    "# now let's do the same for the vulnerabilities index.  Same query\n",
    "response = es.search(index=\"vulnerabilities\", body=query)\n",
    "vuln_project_names = [bucket['key'] for bucket in response['aggregations']['unique_project_names']['buckets']]\n",
    "print(\"\\nUnique project names in vulnerabilities index:\")\n",
    "for project_name in vuln_project_names:\n",
    "    print(project_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch query examples\n",
    "\n",
    "Here are some ideas:\n",
    "\n",
    "* How many Critical, High, Medium, Low vulnerabilities do we have for each docker-image?\n",
    "* What is the precentage of each type for the docker-images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application type count for each of our projects.\n",
      "Project: docker-image|docker.elastic.co/elasticsearch/elasticsearch\n",
      "Apps with vulns: 102\n",
      "Project: docker-image|todolist-goof\n",
      "Apps with vulns: 7\n",
      "\n",
      "\n",
      "Vulnerability type count for each of our projects.\n",
      "Project: docker-image|todolist-goof\n",
      "Critical: 0\n",
      "High: 0\n",
      "Med: 0\n",
      "Project: com.scalesec:vulnado\n",
      "Critical: 4\n",
      "High: 162\n",
      "Med: 68\n",
      "Project: docker-image|docker.elastic.co/elasticsearch/elasticsearch\n",
      "Critical: 0\n",
      "High: 0\n",
      "Med: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Application type count for each of our projects.\")\n",
    "for project_name in app_project_names:\n",
    "    print(f\"Project: {project_name}\")\n",
    "    # do an elasticsearch query for the project_name in the applications index, for the vulnerabilities of each type -\n",
    "    # critical, high, medium, low\n",
    "    # app_critical = es.search(index=\"applications\", query={\"match\": {\"project_name\": project_name, \"severity\": \"critical\"}})\n",
    "    \n",
    "    \n",
    "    app_critical = es.search(index=\"applications\",\n",
    "                             query= {\"bool\": {\n",
    "                                 \"must\" : [\n",
    "                                     { \"range\" : {\n",
    "                                         \"vulns\" : {\"gt\" : 0}\n",
    "                                         }\n",
    "                                     },\n",
    "                                     { \"term\" : {\"projectName.keyword\" : project_name}}\n",
    "                                     ]\n",
    "                                 }},\n",
    "                             size=0,\n",
    "                             aggs={\n",
    "                                 \"vuln_count\" : {\n",
    "                                     \"value_count\" : {\n",
    "                                         \"field\" : \"vulns.keyword\"\n",
    "                                     }\n",
    "                                 }\n",
    "                             },\n",
    "                             )\n",
    "    print(f\"Apps with vulns: {app_critical['hits']['total']['value']}\")\n",
    "\n",
    "print(\"\\n\\nVulnerability type count for each of our projects.\")\n",
    "for unique_project in vuln_project_names:\n",
    "    print(f\"Project: {unique_project}\")\n",
    "    vuln_critical = es.search(index=\"vulnerabilities\",\n",
    "                             query= {\"bool\": {\n",
    "                                 \"must\" : [\n",
    "                                     { \"match\" : {\"severity\" : \"critical\"}},\n",
    "                                     { \"term\" : {\"projectName.keyword\" : unique_project}}\n",
    "                                     ]\n",
    "                                 }},)\n",
    "    print(f\"Critical: {vuln_critical['hits']['total']['value']}\")\n",
    "\n",
    "    vuln_high = es.search(index=\"vulnerabilities\",\n",
    "                             query= {\"bool\": {\n",
    "                                 \"must\" : [\n",
    "                                     { \"match\" : {\"severity\" : \"high\"}},\n",
    "                                     { \"term\" : {\"projectName.keyword\" : unique_project}}\n",
    "                                     ]\n",
    "                                 }},)\n",
    "    print(f\"High: {vuln_high['hits']['total']['value']}\")\n",
    "\n",
    "    vuln_med = es.search(index=\"vulnerabilities\",\n",
    "                             query= {\"bool\": {\n",
    "                                 \"must\" : [\n",
    "                                     { \"match\" : {\"severity\" : \"medium\"}},\n",
    "                                     { \"term\" : {\"projectName.keyword\" : unique_project}}\n",
    "                                     ]\n",
    "                                 }},)\n",
    "    print(f\"Med: {vuln_med['hits']['total']['value']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
