{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The motivation for this Jupyter noteboook is to take snyk output, import it into Elasticsearch, and render some interesting output.  Ordinarily, I would use the snyk REST API, but that is not available on the free tier.  Web scraping is difficult because the site uses OAUTH, and getting Selenium (to render Javascript) working with modified Headers is a tricky exercise.  At least trickier than I had time to solve.\n",
    "\n",
    "In this Jupyter notebook, I take the output from snyk CLI commands and process them with Python.  Part of this processing is to get data into ElasticSearch.  I have a 2-node ElasticSearch cluster on my Ubuntu machine, for some experiments and training.\n",
    "\n",
    "Once the data is in ElasticSearch, I'll do some transformations both with ES and with Python.  For example, on ES I will try some queries and some data representation.  On Python, I'll use the search capabilities of the library to create dataframes and maybe even some plots.\n",
    "\n",
    "I use two types of files.  I'll lead with JSON files from the SnykCLI and use those as inputs into ES.  I'll also generate sarif files.  I expect most work to happen with the JSON files.\n",
    "\n",
    "I tested the following on\n",
    "- Ubuntu 24 LTS \n",
    "- Elasticsearch 8.15\n",
    "- Python 3.12\n",
    "- Snyk CLI 1.1293.1\n",
    "\n",
    "These are the repositories that I used to generate the output files:\n",
    "- git@github.com:marcoman/vulnado.git\n",
    "- git@github.com:marcoman/java-goof.git\n",
    "- git@github.com:marcoman/goof.git\n",
    "\n",
    "These are the containers I used to generate output files:\n",
    "- docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "- A local container built from https://github.com/marcoman/goof/tree/develop/todolist\n",
    "\n",
    "For reference, these are some of the commands I ran to get my files:\n",
    "\n",
    "```bash\n",
    "snyk container test --json --json-file-output=container-elastic.json --app-vulns docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "snyk container test --sarif --sarif-file-output=container-elastic.sarif --app-vulns docker.elastic.co/elasticsearch/elasticsearch:8.15.1\n",
    "\n",
    "snyk container test --json --json-file-output=container-todolist-goof.json --app-vulns todolist-goof:latest\n",
    "snyk container test --sarif --sarif-file-output=container-todolist-goof.sarif --app-vulns todolist-goof:latest \n",
    "\n",
    "snyk test --json-file-output=os-goof-todolist.json --json\n",
    "snyk test --json-file-output=os-java-goof.json --json\n",
    "snyk test --json-file-output=os-vulnado.json --json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "import urllib3\n",
    "\n",
    "# This call disables the InsecureRequestWarning for unverified HTTPS requests\n",
    "# This is a common workaround for disabling SSL certificate verification in Python\n",
    "# It should not be used in production environments, as it can lead to security risks\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up our files and start to examine them.  The JSON files come in heavy, and we may have to reduce or only load in a subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to load in files into a variable.  \n",
    "# We *might* use the file contents as-is, but more likely just within a JSON object.\n",
    "json_container_elastic = None\n",
    "json_container_todolist = None\n",
    "json_os_goof_todolist = None\n",
    "json_os_jav_goof = None\n",
    "json_os_vulnado = None\n",
    "\n",
    "## Read in the file elastic.json as a json structure\n",
    "with open('datafiles/container-elastic.json') as f:\n",
    "    json_elastic = json.load(f)\n",
    "\n",
    "## Read in the file todolist-goof.json as a json structure.\n",
    "with open('datafiles/container-todolist-goof.json') as f:\n",
    "    json_todolist = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part gets our envrionment variables to collect our API credentials.  In my environment, I set these values envvars to help me avoid adding them to the code.  My Elasticsearch server is on my computer, and it is not likely the world will be attacking it.  Still, it is a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ELASTIC_API_URL = os.environ.get('ELASTIC_API_URL')\n",
    "ELASTIC_API_KEY = os.environ.get('ELASTIC_API_KEY')\n",
    "#THe authorization headers are by username + password\n",
    "headers = {\n",
    "    'Authorization': f'ApiKey {ELASTIC_API_KEY}'\n",
    "}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://172.29.213.51:9200/\n"
     ]
    }
   ],
   "source": [
    "print(ELASTIC_API_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Elasticsearch connection\n",
    "\n",
    "As a test, let's see if we access the ES server via a requests call.  This is different from using the ES library, which we'll test later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your products are: \n",
      "{'products': {'aliases': {}, 'mappings': {'properties': {'created': {'type': 'date', 'format': 'yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis'}, 'description': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'id': {'type': 'long'}, 'in_stock': {'type': 'long'}, 'is_active': {'type': 'boolean'}, 'name': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'price': {'type': 'long'}, 'sold': {'type': 'long'}, 'tages': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}, 'tags': {'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}}}, 'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '2', 'provided_name': 'products', 'creation_date': '1726425825891', 'number_of_replicas': '2', 'uuid': 'tPcO96JhRLqzI7bhDZSGXQ', 'version': {'created': '8512000'}}}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "## Read the products from the Elastic Server.  This is a GET request to /products\n",
    "def get_products():\n",
    "    url = f\"{ELASTIC_API_URL}/products\"\n",
    "    \n",
    "    # We specify verify=False to match curl's --insecure flag\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "products = get_products()\n",
    "print (f'Your products are: \\n{products}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the snyk cli output files. \n",
    "\n",
    "The general structure of the json file is below.  These are organized with a few differet top-level lists, and we'll spend most of our time focusing on the `vulnerabilities` and `applications` lists.  As I work over the examples, I am expecting to use the `projectName` and `path` as identifiers or query criteria.  This means I'm likely to add all vulnerabilities and application to their respective indicies, and the query will be my filter.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"vulnerabilities\": [],\n",
    "    ...\n",
    "    \"summary\" : \"\",\n",
    "    \"projectName\" : \"\",\n",
    "    \"path\" : \"\",\n",
    "    \"applications\" : [\n",
    "        {\n",
    "            \"projectName\":\"\",\n",
    "            \"dependencyCount\":\"\",\n",
    "            \"displayTargetFile\":\"\",\n",
    "            \"targetFile\":\"\",\n",
    "            \"path\":\"\",\n",
    "            \"packageManager\":\"\",\n",
    "            \"summary\" : \"\",\n",
    "            \"vulnerabilities\":[]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Elasticsearch client and use the API key to log on.\n",
    "\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(ELASTIC_API_URL, api_key=ELASTIC_API_KEY, verify_certs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index applications does not exist\n",
      "Index vulnerabilities does not exist\n"
     ]
    }
   ],
   "source": [
    "# Clean up indicies to start clean\n",
    "# Delete only if the indicies are present.\n",
    "\n",
    "if es.indices.exists(index='applications'):\n",
    "    print(\"Deleting index applications\")\n",
    "    res = es.indices.delete(index='applications')\n",
    "    print(res)\n",
    "else:\n",
    "    print(\"Index applications does not exist\")\n",
    "\n",
    "if es.indices.exists(index='vulnerabilities'):\n",
    "    print(\"Deleting index vulnerabilities\")\n",
    "    res = es.indices.delete(index='vulnerabilities')\n",
    "    print(res)\n",
    "else:\n",
    "    print(\"Index vulnerabilities does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indicies for our test\n",
    "\n",
    "I create indicies for both `applications` and `vulnerabilities` explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Elasticsearch index named \"applications\"\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=\"applications\"):\n",
    "    response = es.indices.create(index=\"applications\", body=index_settings)\n",
    "\n",
    "if not es.indices.exists(index=\"vulnerabilities\"):\n",
    "    response = es.indices.create(index=\"vulnerabilities\", body=index_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the indicies\n",
    "\n",
    "The easiest solution is to iterate through the different JSON files and add their data to the indicies.  As I add more, I will automate this even better for the available JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on Elasticsearch container\n",
      "There are 94 applications\n",
      "There are 104 vulnerabilities\n",
      "Operating on Java TODO List container\n",
      "There are 7 applications\n",
      "There are 2293 vulnerabilities\n"
     ]
    }
   ],
   "source": [
    "json_files = [[\"Elasticsearch container\", json_elastic],\n",
    "              [\"Java TODO List container\", json_todolist],\n",
    "              ]\n",
    "\n",
    "def iterate_through_json(jsonfile):\n",
    "    # we want to iterate and report on two lists inside of the Json body named jsonfile.\n",
    "    # The first is named applications, and the second is named vulnerabilities.  \n",
    "    # These two lists are independent and at the same level\n",
    "    # Read through each and print out their contents\n",
    "    print(f'Operating on {jsonfile[0]}')\n",
    "    i = 0\n",
    "    for app in jsonfile[1]['applications']:\n",
    "        # print(app)\n",
    "        # Now load each app named \"app\" as a new document in ElasticSearch into the index named \"applications\"\n",
    "        # There is variation in the records and I need to adjust how they are stored.  For example, the upgradePath is empty or contains values.async_search\n",
    "        # For this part, I'll create a new record that is just a subset of the original.\n",
    "        newapp = {\n",
    "            \"projectName\": app['projectName'],\n",
    "            \"targgetFile\": app['targetFile'],\n",
    "            \"displayTargetFile\": app['displayTargetFile'],\n",
    "            \"id\" : i,\n",
    "        }\n",
    "        es.index(index=\"applications\", document=newapp)\n",
    "        i += 1\n",
    "    print(f'There are {i} applications')\n",
    "\n",
    "    i = 0        \n",
    "    for vuln in jsonfile[1]['vulnerabilities']:\n",
    "        # print(vuln)\n",
    "        newvuln = {\n",
    "            \"id\": vuln['id'],\n",
    "            \"CVSSv3\": vuln['CVSSv3'],\n",
    "            \"severity\": vuln['severity'],\n",
    "            \"cvssScore\": vuln['cvssScore'],\n",
    "            \"description\": vuln['description'],\n",
    "            \"packageName\": vuln['packageName'],\n",
    "        }\n",
    "        es.index(index=\"vulnerabilities\", document=newvuln)\n",
    "        i += 1\n",
    "    print(f'There are {i} vulnerabilities')\n",
    "\n",
    "for jsonfile in json_files:\n",
    "    iterate_through_json(jsonfile=jsonfile)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
